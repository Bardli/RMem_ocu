{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning AOT+ Model on Extracted Frames Dataset\n",
    "\n",
    "This notebook provides a template for fine-tuning AOT+ (or related) models on the `EXTRACTED_FRAMES` dataset. The `EXTRACTED_FRAMES` dataset is designed to load images and their corresponding polygon annotations from JSON files, making it suitable for custom datasets where annotations are in this format.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Configure parameters (experiment name, model, paths, hyperparameters).\n",
    "2. Construct the training command.\n",
    "3. Set environment variables (like `CUDA_VISIBLE_DEVICES`).\n",
    "4. Execute the training script (`tools/train.py`).\n",
    "\n",
    "**Before Running:**\n",
    "- Ensure your `EXTRACTED_FRAMES` dataset is correctly placed (default assumption: `./extracted_frames/`).\n",
    "- **Crucially, update the `pretrained_model_path` variable to point to your pretrained model weights.**\n",
    "- Adjust other parameters like `model`, `batch_size`, `total_steps`, and GPU settings as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Settings\n",
    "exp_name = \"finetune_extracted_notebook\"\n",
    "model = \"r50_aotl\"  # Options: \"aott\", \"aots\", \"aotb\", \"aotl\", \"r50_deaotl\", \"swinb_aotl\", etc.\n",
    "stage = \"default\"    # Adjust if a specific fine-tuning stage or pretrained model stage is needed\n",
    "dataset_name = \"EXTRACTED_FRAMES\"\n",
    "\n",
    "# Path to your pretrained model (VERY IMPORTANT - UPDATE THIS PATH)\n",
    "pretrained_model_path = \"\"  # e.g., \"./pretrain_models/r50_aotl.pth\" or \"/path/to/your/model.pth\"\n",
    "\n",
    "# GPU Configuration\n",
    "gpu_num = 1       # Number of GPUs to use\n",
    "devices = \"0\"     # Comma-separated list of GPU IDs (e.g., \"0\" or \"0,1\")\n",
    "\n",
    "# Training Hyperparameters\n",
    "batch_size = 2    # Adjust based on your GPU memory\n",
    "total_steps = 10000 # Total number of training steps\n",
    "use_amp = True    # Enable Automatic Mixed Precision (if supported)\n",
    "fix_random_seed = True # For reproducibility\n",
    "\n",
    "# Optional: Log directory\n",
    "log_directory = f\"./logs_{exp_name}\" # Example log directory\n",
    "\n",
    "# Path to the training script\n",
    "train_script_path = \"tools/train.py\"\n",
    "\n",
    "# Basic check for pretrained_model_path (encourage user to fill it)\n",
    "if not pretrained_model_path:\n",
    "    print(\"WARNING: `pretrained_model_path` is not set. Fine-tuning usually requires a pretrained model.\")\n",
    "    print(\"Please update it to the path of your .pth model file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Constructing the Training Command\n",
    "\n",
    "This cell assembles the command-line arguments for `tools/train.py` based on the parameters defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = [\"python\", train_script_path]\n",
    "command.extend([\"--exp_name\", exp_name])\n",
    "command.extend([\"--stage\", stage])\n",
    "command.extend([\"--model\", model])\n",
    "command.extend([\"--datasets\", dataset_name])\n",
    "command.extend([\"--gpu_num\", str(gpu_num)])\n",
    "command.extend([\"--batch_size\", str(batch_size)])\n",
    "command.extend([\"--total_step\", str(total_steps)])\n",
    "\n",
    "if use_amp:\n",
    "    command.append(\"--amp\")\n",
    "if fix_random_seed:\n",
    "    command.append(\"--fix_random\")\n",
    "\n",
    "if pretrained_model_path:  # Only add if a path is provided\n",
    "    command.extend([\"--pretrained_path\", pretrained_model_path])\n",
    "else:\n",
    "    print(\"INFO: Proceeding without a specified `pretrained_model_path`. The model might train from scratch or use default weights if defined in its config.\")\n",
    "\n",
    "# To use a custom log directory, uncomment the following line:\n",
    "# command.extend([\"--log\", log_directory])\n",
    "\n",
    "print(\"Constructed command:\")\n",
    "# Pretty print for readability\n",
    "import shlex\n",
    "print(shlex.join(command))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting CUDA_VISIBLE_DEVICES\n",
    "\n",
    "To ensure the training script uses the specified GPUs, we set the `CUDA_VISIBLE_DEVICES` environment variable. This is particularly important in multi-GPU systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = devices\n",
    "print(f\"CUDA_VISIBLE_DEVICES set to: {os.environ['CUDA_VISIBLE_DEVICES']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Executing the Training Command\n",
    "\n",
    "This cell runs the training script using `subprocess.run`. This method is generally preferred over `!python` (shell command) in notebooks because it offers better control over the process, allows capturing output programmatically, and integrates more smoothly with Python variables.\n",
    "\n",
    "**Note:** Training can take a significant amount of time depending on the dataset size, model complexity, number of steps, and hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "print(\"\\nStarting training... This may take a while.\")\n",
    "print(f\"Executing command: {shlex.join(command)}\")\n",
    "print(f\"Output will be streamed below if running in a typical Jupyter environment that handles subprocess output well.\\n\")\n",
    "\n",
    "# Using subprocess.run for better control. \n",
    "# For live output in Jupyter, Popen might be better, but requires more complex handling.\n",
    "# result = subprocess.run(command, capture_output=True, text=True, check=False)\n",
    "\n",
    "# Simpler alternative for direct output to notebook (less control over capture):\n",
    "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, universal_newlines=True)\n",
    "\n",
    "if process.stdout:\n",
    "    for line in iter(process.stdout.readline, ''):\n",
    "        print(line, end='')\n",
    "    process.stdout.close()\n",
    "\n",
    "return_code = process.wait()\n",
    "\n",
    "if return_code == 0:\n",
    "    print(\"\\n--- Training successfully completed. ---\")\n",
    "else:\n",
    "    print(f\"\\n--- Training exited with code: {return_code} ---\")\n",
    "\n",
    "# If you used result = subprocess.run(command, capture_output=True, text=True):\n",
    "# print(\"\\n--- Training Output (stdout) ---\")\n",
    "# print(result.stdout)\n",
    "# if result.stderr:\n",
    "#     print(\"\\n--- Training Errors (stderr) ---\")\n",
    "#     print(result.stderr)\n",
    "# print(\"\\nTraining finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Important Notes\n",
    "\n",
    "*   **Update `pretrained_model_path`**: This is the most critical parameter for fine-tuning. Ensure it points to a valid `.pth` model file.\n",
    "*   **Adjust Parameters**: Modify `batch_size` according to your GPU memory. Change `total_steps` based on how long you want to fine-tune. Select the correct `model` architecture. Update `gpu_num` and `devices` for your specific hardware setup.\n",
    "*   **Logs**: If you uncommented the `--log` argument in the command construction, training logs (including print statements, TensorBoard files, and saved checkpoints) will be saved to the specified directory (e.g., `./logs_finetune_extracted_notebook/`).\n",
    "*   **Dataset Location**: The `ExtractedFramesTrain` dataset loader defaults to looking for data in `./extracted_frames/`. Ensure your images and JSON annotations are there.\n",
    "*   **Kernel Interrupts**: If you interrupt the kernel during the `subprocess.run` execution, it might not immediately terminate the child process (`tools/train.py`). You might need to manually stop it if it continues running in the background."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
