{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning AOT+ Model on Extracted Frames Dataset\n",
    "\n",
    "This notebook provides a template for fine-tuning AOT+ (or related) models on the `EXTRACTED_FRAMES` dataset. The `EXTRACTED_FRAMES` dataset is designed to load images and their corresponding polygon annotations from JSON files, making it suitable for custom datasets where annotations are in this format.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Configure parameters (experiment name, model, paths, hyperparameters).\n",
    "2. Load and build the configuration object (`cfg`).\n",
    "3. Set up the environment and instantiate the `Trainer`.\n",
    "4. Execute the training directly in the notebook.\n",
    "\n",
    "**Before Running:**\n",
    "- Ensure your `EXTRACTED_FRAMES` dataset is correctly placed (default assumption: `./extracted_frames/`).\n",
    "- **Crucially, update the `pretrained_model_path` variable to point to your pretrained model weights.**\n",
    "- Adjust other parameters like `model_name_str`, `batch_size`, `total_steps`, and GPU settings as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "655715d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Paths and Imports\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import importlib # For get_config replication\n",
    "import torch\n",
    "import numpy as np\n",
    "import json # For pretty printing config\n",
    "\n",
    "# Add project root to sys.path to allow importing aot_plus modules\n",
    "# Assuming the notebook is in aot_plus/\n",
    "if '.' not in sys.path:\n",
    "    sys.path.insert(0, '.') # Current directory (aot_plus/)\n",
    "# To import from 'aot_plus.networks' etc., 'aot_plus' parent dir should be in path.\n",
    "# If notebook is in 'aot_plus/', then '..' is its parent.\n",
    "# If 'aot_plus' is the root of the project, then '.' is correct for files inside 'aot_plus'.\n",
    "# Let's assume the notebook is run from the 'aot_plus' directory itself or its parent.\n",
    "project_root = os.path.abspath('.') # If notebook is in aot_plus/\n",
    "if 'aot_plus' not in project_root.split(os.sep)[-1]: # If current dir is not aot_plus\n",
    "    # Try to find aot_plus directory if notebook is in parent like 'AOT-Tracker-plus/'\n",
    "    if os.path.isdir(os.path.join(project_root, 'aot_plus')):\n",
    "        project_root = os.path.join(project_root, 'aot_plus')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Adjust path if running from parent directory of aot_plus (e.g. AOT-Tracker-plus)\n",
    "if os.path.basename(os.getcwd()) != 'aot_plus' and os.path.exists('aot_plus'):\n",
    "    print(\"Changing current working directory to 'aot_plus'\")\n",
    "    os.chdir('aot_plus')\n",
    "    if '.' not in sys.path:\n",
    "        sys.path.insert(0, '.')\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Sys.path includes: {sys.path[:3]}\") # Show top few for brevity\n",
    "\n",
    "# Import necessary components from the aot_plus project\n",
    "try:\n",
    "    from networks.managers.trainer import Trainer\n",
    "    from utils.utils import Tee, copy_codes, make_log_dir \n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Error importing project modules: {e}\")\n",
    "    print(\"Please ensure the notebook is in the 'aot_plus' directory, or the 'aot_plus' directory is in PYTHONPATH.\")\n",
    "    print(\"Alternatively, run the notebook from the parent directory of 'aot_plus'.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "441f4f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Parameters\n",
    "\n",
    "# Experiment Settings\n",
    "exp_name = \"finetune_extracted_notebook\"\n",
    "model_name_str = \"r50_aotl\"  # Actual model identifier used by config system (e.g., \"aott\", \"r50_aotl\")\n",
    "stage_str = \"default\"        # Stage for get_config (e.g., \"default\", \"pre\", \"ytb\")\n",
    "dataset_name = \"EXTRACTED_FRAMES\" # Will be set in cfg.DATASETS\n",
    "\n",
    "# Path to your pretrained model (VERY IMPORTANT - UPDATE THIS PATH)\n",
    "# This will be set to cfg.PRETRAIN_MODEL\n",
    "pretrained_model_path = \"\"  # e.g., \"./pretrain_models/r50_aotl.pth\" or \"/path/to/your/model.pth\"\n",
    "\n",
    "# GPU Configuration (for single GPU execution in notebook)\n",
    "gpu_id = 0 # Python variable for the GPU ID\n",
    "enable_amp = True\n",
    "\n",
    "# Training Hyperparameters (will be set in cfg)\n",
    "batch_size = 2    # Adjust based on your GPU memory\n",
    "total_steps = 10000 # Example, adjust as needed for fine-tuning\n",
    "fix_random_seed = True\n",
    "\n",
    "# Log directory (base directory for logs)\n",
    "log_base_dir = os.path.join(os.getcwd(), f\"logs_notebook_{exp_name}\")\n",
    "\n",
    "# --- Parameters previously handled by argparse in train.py ---\n",
    "# These will be used to populate the cfg object\n",
    "\n",
    "dist_start_gpu = gpu_id  # cfg.DIST_START_GPU (Used by Trainer for self.gpu if not distributed)\n",
    "train_gpus = 1           # cfg.TRAIN_GPUS (Set to 1 for notebook's single GPU context)\n",
    "train_batch_size = batch_size # cfg.TRAIN_BATCH_SIZE\n",
    "\n",
    "# cfg.PRETRAIN_MODEL (Path to pretrained weights) - already defined as pretrained_model_path\n",
    "\n",
    "# cfg.TRAIN_LR (If not set, uses default from config. Set to -1 to use default from config file)\n",
    "train_lr = -1.0 \n",
    "\n",
    "train_total_steps = total_steps # cfg.TRAIN_TOTAL_STEPS\n",
    "\n",
    "# cfg.TRAIN_START_STEP (Usually 0 for new fine-tuning, or from a resumed checkpoint)\n",
    "train_start_step = 0 \n",
    "\n",
    "dist_url = '' # cfg.DIST_URL (Not used for single GPU)\n",
    "\n",
    "# cfg.AMP (Handled by enable_amp for Trainer) - already defined as enable_amp\n",
    "# cfg.LOG (Base log directory) - already defined as log_base_dir\n",
    "# cfg.FIX_RANDOM - already defined as fix_random_seed\n",
    "\n",
    "print(f\"Experiment: {exp_name}, Model: {model_name_str}, Stage: {stage_str}\")\n",
    "if not pretrained_model_path:\n",
    "    print(\"WARNING: pretrained_model_path is not set. Training will start from scratch unless the stage/model config loads one by default.\")\n",
    "else:\n",
    "    print(f\"Pretrained model: {pretrained_model_path}\")\n",
    "print(f\"Log directory will be under: {log_base_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d0825d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Configuration\n",
    "from datetime import datetime\n",
    "# Replicate get_config function from tools/get_config.py\n",
    "def get_config_notebook(stage_name, exp_name_cfg, model_cfg_name):\n",
    "    try:\n",
    "        # Construct module name, assuming 'configs' is a package in the project root\n",
    "        module_name = 'configs.' + stage_name\n",
    "        engine_config_module = importlib.import_module(module_name)\n",
    "        \n",
    "        if hasattr(engine_config_module, 'EngineConfig'):\n",
    "            return engine_config_module.EngineConfig(exp_name_cfg, model_cfg_name)\n",
    "        elif hasattr(engine_config_module, 'DefaultEngineConfig'): # Common for 'default' stage\n",
    "             return engine_config_module.DefaultEngineConfig(exp_name_cfg, model_cfg_name)\n",
    "        else:\n",
    "            raise AttributeError(f\"Neither 'EngineConfig' nor 'DefaultEngineConfig' class found in module '{module_name}'.\")\n",
    "\n",
    "    except ModuleNotFoundError:\n",
    "        print(f\"ERROR: Stage configuration module '{module_name}.py' not found.\")\n",
    "        print(\"Please ensure 'stage_str' in the notebook refers to a valid config file (e.g., 'default', 'pre', 'ytb').\")\n",
    "        raise\n",
    "    except AttributeError as e:\n",
    "        print(f\"ERROR: Configuration class not found or attribute error: {e}\")\n",
    "        raise\n",
    "\n",
    "cfg = get_config_notebook(stage_str, exp_name, model_name_str)\n",
    "\n",
    "# Apply settings from notebook variables (previously argparse)\n",
    "cfg.DATASETS = [dataset_name] # Must be a list\n",
    "cfg.DIST_START_GPU = dist_start_gpu\n",
    "cfg.TRAIN_GPUS = train_gpus # Set to 1 for single GPU\n",
    "\n",
    "if train_batch_size > 0: cfg.TRAIN_BATCH_SIZE = train_batch_size\n",
    "if pretrained_model_path: cfg.PRETRAIN_MODEL = pretrained_model_path\n",
    "if train_lr > 0: cfg.TRAIN_LR = train_lr # Only override if train_lr is positive\n",
    "if train_total_steps > 0: cfg.TRAIN_TOTAL_STEPS = train_total_steps\n",
    "if train_start_step >= 0: cfg.TRAIN_START_STEP = train_start_step # Allow 0\n",
    "\n",
    "cfg.LOG = log_base_dir # Base log directory for make_log_dir\n",
    "cfg.FIX_RANDOM = fix_random_seed\n",
    "# cfg.AMP = enable_amp # Trainer takes enable_amp directly, cfg.AMP might not be used by Trainer\n",
    "\n",
    "# Initialize directories which might be done in EngineConfig's init_dir()\n",
    "if hasattr(cfg, 'init_dir') and callable(cfg.init_dir):\n",
    "    cfg.init_dir() # This will set up DIR_LOG, DIR_CKPT etc.\n",
    "else:\n",
    "    print(\"Warning: cfg.init_dir() method not found. Log directories might not be auto-created by config.\")\n",
    "    # Manually set some essential ones if not set by init_dir\n",
    "    if not hasattr(cfg, 'DIR_LOG'): cfg.DIR_LOG = os.path.join(cfg.LOG, cfg.EXP_NAME)\n",
    "    if not hasattr(cfg, 'DIR_CKPT'): cfg.DIR_CKPT = os.path.join(cfg.DIR_LOG, 'ckpt')\n",
    "\n",
    "# Use make_log_dir to ensure the specific log directory for this run is created\n",
    "def safe_make_log_dir(log_dir, name):\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    new_log_dir = os.path.join(log_dir, f\"{ts}_{name}\")\n",
    "    if os.path.isdir(new_log_dir):\n",
    "        raise Exception(f\"{new_log_dir} already exist ! abort ...\")\n",
    "    os.makedirs(new_log_dir)\n",
    "    return new_log_dir\n",
    "# cfg.EXP_NAME is used by make_log_dir to create the final experiment log path\n",
    "current_log_dir = safe_make_log_dir(cfg.LOG, cfg.EXP_NAME) \n",
    "# Update cfg.DIR_LOG to the fully qualified path returned by make_log_dir\n",
    "cfg.DIR_LOG = current_log_dir \n",
    "if not hasattr(cfg, 'DIR_TB_LOG'): cfg.DIR_TB_LOG = os.path.join(cfg.DIR_LOG, 'tensorboard') # Common practice\n",
    "if not os.path.exists(cfg.DIR_TB_LOG): os.makedirs(cfg.DIR_TB_LOG, exist_ok=True)\n",
    "if not os.path.exists(cfg.DIR_CKPT): os.makedirs(cfg.DIR_CKPT, exist_ok=True)\n",
    "\n",
    "# Optional: copy_codes(current_log_dir)\n",
    "\n",
    "# Save config to log directory (simple JSON dump for notebook)\n",
    "try:\n",
    "    config_save_path = os.path.join(current_log_dir, \"config_notebook.json\")\n",
    "    # Create a serializable version of cfg (excluding non-serializable items if any)\n",
    "    cfg_dict = {k: v for k, v in cfg.__dict__.items() if not k.startswith('__') and not callable(v)}\n",
    "    with open(config_save_path, 'w') as f:\n",
    "        json.dump(cfg_dict, f, indent=4)\n",
    "    print(f\"Configuration saved to {config_save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving config: {e}\")\n",
    "\n",
    "print(f\"Configuration loaded. Log directory for this run: {cfg.DIR_LOG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e744367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_17380\\3187809420.py\", line 38, in <module>\n",
      "    trainer.sequential_training()\n",
      "  File \"d:\\baidu\\medventions file\\RMEM\\RMem_ocu\\aot_plus\\networks\\managers\\trainer.py\", line 453, in sequential_training\n",
      "    if train_sampler is not None:\n",
      "AttributeError: 'NoneType' object has no attribute 'set_epoch'\n"
     ]
    }
   ],
   "source": [
    "# Training Execution\n",
    "\n",
    "# This cell adapts the main_worker function from tools/train.py\n",
    "\n",
    "current_gpu_id = gpu_id # Defined in config cell\n",
    "torch.cuda.set_device(current_gpu_id)\n",
    "print(f\"Using GPU: {current_gpu_id}\")\n",
    "\n",
    "if cfg.FIX_RANDOM:\n",
    "    # Use a fixed seed for notebook, or derive from gpu_id if preferred\n",
    "    random_seed = 42 \n",
    "    print(f\"Fixing random seed to {random_seed}\")\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "    random.seed(random_seed + 1)\n",
    "    np.random.seed(random_seed + 2)\n",
    "    torch.manual_seed(random_seed + 3)\n",
    "    torch.cuda.manual_seed(random_seed + 4)\n",
    "    torch.cuda.manual_seed_all(random_seed + 5)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Tee for logging output to a file in addition to notebook cell output\n",
    "# Note: This will capture print statements. Trainer's internal logging might also write to files.\n",
    "log_file_path = os.path.join(cfg.DIR_LOG, \"print_notebook.log\")\n",
    "original_stdout = sys.stdout\n",
    "tee = Tee(log_file_path)\n",
    "sys.stdout = tee\n",
    "print(f\"Notebook output is being logged to: {log_file_path}\")\n",
    "\n",
    "print(f\"Initiating Trainer on GPU {current_gpu_id} (rank 0 for notebook context)\")\n",
    "# rank is 0 as we are not using mp.spawn for distributed training in the notebook\n",
    "cfg.DIST_ENABLE = False\n",
    "cfg.MODEL_ENCODER_PRETRAIN = './pretrain_models/aotplus_R50_DeAOTL_Temp_pe_Slot_4_ema_20000.pth'\n",
    "trainer = Trainer(rank=0, cfg=cfg, enable_amp=enable_amp) \n",
    "\n",
    "print(\"Starting training...\")\n",
    "try:\n",
    "    trainer.sequential_training()\n",
    "    print(\"Training finished successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Restore original stdout and close the Tee logger\n",
    "    sys.stdout = original_stdout\n",
    "    tee.close()\n",
    "    print(f\"Restored stdout. Log file saved at: {log_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e8bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法一：使用 os 模块\n",
    "import os\n",
    "\n",
    "dir_path = '../extracted_frames/'\n",
    "try:\n",
    "    print(f\"目录 {dir_path} 下的文件：\")\n",
    "    for name in os.listdir(dir_path):\n",
    "        fullpath = os.path.join(dir_path, name)\n",
    "        if os.path.isfile(fullpath):\n",
    "            print(name)\n",
    "except FileNotFoundError:\n",
    "    print(f\"目录 {dir_path} 不存在\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "The `dataloaders` have been configured so that the `ExtractedFramesTrain` dataset can also be used for evaluation. If you wish to evaluate your fine-tuned model on the `extracted_frames` data (or a subset of it), you would typically:\n",
    "\n",
    "1.  **Adapt the Configuration (`cfg`):**\n",
    "    *   Ensure `cfg.EVAL_DATASETS` (or a similar configuration attribute used by evaluation scripts/logic) is set to `[\"EXTRACTED_FRAMES\"]`.\n",
    "    *   Set `cfg.TEST_BATCH_SIZE` as needed.\n",
    "    *   Specify the checkpoint of your fine-tuned model for evaluation (e.g., `cfg.TEST_CKPT_PATH` or similar).\n",
    "\n",
    "2.  **Evaluation Logic:**\n",
    "    *   The `Trainer` class (`networks.managers.trainer.Trainer`) might have evaluation methods, or you might need to adapt logic from `tools/eval.py`.\n",
    "    *   This would involve:\n",
    "        *   Loading the fine-tuned model weights.\n",
    "        *   Instantiating an evaluation dataloader: `eval_loader = build_eval_dataloader(cfg, \"EXTRACTED_FRAMES\", split=\"val\")` (assuming \"val\" split is relevant or you use all data).\n",
    "        *   Iterating through the `eval_loader` and running inference with the model.\n",
    "        *   Calculating evaluation metrics (e.g., IoU).\n",
    "\n",
    "*This notebook currently focuses on training. Full evaluation script integration is a separate step.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Important Notes\n",
    "\n",
    "*   **Update `pretrained_model_path`**: This is the most critical parameter for fine-tuning. Ensure it points to a valid `.pth` model file.\n",
    "*   **Adjust Parameters**: Modify `batch_size` according to your GPU memory. Change `total_steps` based on how long you want to fine-tune. Select the correct `model_name_str` and `stage_str` matching your project's configurations in `aot_plus/configs/`.\n",
    "*   **Logs**: Training logs, print statements (mirrored to `print_notebook.log`), TensorBoard files, and saved checkpoints will be saved to the directory specified in `cfg.DIR_LOG` (e.g., `./logs_notebook_finetune_extracted_notebook/finetune_extracted_notebook/`).\n",
    "*   **Dataset Location**: The `ExtractedFramesTrain` dataset loader defaults to looking for data in `./extracted_frames/`. Ensure your images and JSON annotations are there.\n",
    "*   **Kernel Interrupts**: If you interrupt the kernel, the training process should stop. The `finally` block in the training cell attempts to restore standard output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvlab_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
