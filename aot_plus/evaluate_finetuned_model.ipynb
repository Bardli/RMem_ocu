{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and User-Defined Paths\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2 # For video writing and image manipulation\n",
    "import json # For config display\n",
    "from PIL import Image # For mask saving if needed, or consistency\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add project root to sys.path if not already there\n",
    "# Assuming notebook is in aot_plus/ or its parent directory\n",
    "if '.' not in sys.path:\n",
    "    sys.path.insert(0, '.')\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..')) if os.path.basename(os.getcwd()) == 'aot_plus' else os.path.abspath('.')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "# Ensure 'aot_plus' is the CWD if running from parent\n",
    "if os.path.basename(os.getcwd()) != 'aot_plus' and os.path.exists('aot_plus'):\n",
    "    print(\"Changing CWD to 'aot_plus'\")\n",
    "    os.chdir('aot_plus')\n",
    "\n",
    "# --- User-configurable paths ---\n",
    "# !!! IMPORTANT: User needs to set this path to their fine-tuned model !!!\n",
    "FINETUNED_MODEL_CKPT_PATH = \"./results/finetune_extracted_notebook_R50_AOTL_Temp_pe_Slot_4/default/ckpt/save_step_10000.pth\" # Example path\n",
    "OUTPUT_VIDEO_DIR = \"./evaluation_videos\" # Directory to save output videos and masks\n",
    "EVALUATION_DATA_ROOT = \"./extracted_frames/\" # Path to the evaluation data (e.g., extracted frames)\n",
    "\n",
    "# --- GPU Configuration ---\n",
    "GPU_ID = 0\n",
    "\n",
    "# --- Create output directory ---\n",
    "os.makedirs(OUTPUT_VIDEO_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Finetuned model checkpoint: {FINETUNED_MODEL_CKPT_PATH}\")\n",
    "print(f\"Output video directory: {OUTPUT_VIDEO_DIR}\")\n",
    "print(f\"Evaluation data root: {EVALUATION_DATA_ROOT}\")\n",
    "if not os.path.exists(FINETUNED_MODEL_CKPT_PATH):\n",
    "    print(f\"WARNING: Finetuned model checkpoint not found at {FINETUNED_MODEL_CKPT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Configuration\n",
    "from tools.get_config import get_config # Re-using get_config from tools\n",
    "from utils.utils import Tee, make_log_dir # For potential logging setup (optional for notebook)\n",
    "\n",
    "# --- Basic Configuration Parameters (can be adjusted) ---\n",
    "# These should ideally match the training config under which the model was fine-tuned,\n",
    "# especially model-specific parts.\n",
    "# For evaluation, we might load a default config and then override specific test parameters.\n",
    "EXP_NAME_FOR_CONFIG = \"default_eval\" # Can be generic for eval\n",
    "MODEL_NAME_STR_FOR_CONFIG = \"r50_aotl\" # Should match the fine-tuned model's architecture\n",
    "STAGE_STR_FOR_CONFIG = \"default\" # Or the stage used for training if it affects model structure\n",
    "\n",
    "# Load base configuration\n",
    "cfg = get_config(STAGE_STR_FOR_CONFIG, EXP_NAME_FOR_CONFIG, MODEL_NAME_STR_FOR_CONFIG)\n",
    "\n",
    "# --- Override with Evaluation-Specific Settings ---\n",
    "cfg.TEST_CKPT_PATH = FINETUNED_MODEL_CKPT_PATH\n",
    "cfg.TEST_GPU_ID = GPU_ID\n",
    "cfg.TEST_GPU_NUM = 1 # Single GPU for notebook evaluation\n",
    "cfg.DIST_ENABLE = False # Ensure non-distributed mode\n",
    "\n",
    "# Settings for AOTInferEngine (can be adjusted if needed)\n",
    "cfg.TEST_LONG_TERM_MEM_GAP = 9999 # From AOTInferEngine default in evaluator\n",
    "cfg.MODEL_MAX_OBJ_NUM = cfg.MODEL_MAX_OBJ_NUM # Ensure this is consistent with training\n",
    "\n",
    "# Ensure the dataset config for ExtractedFrames is present if we use it by name later\n",
    "# This part is mostly for consistency if build_eval_dataset looks for it.\n",
    "# For direct instantiation of ExtractedFramesTrain, we pass params directly.\n",
    "if 'EXTRACTED_FRAMES' not in cfg.DATASET_CONFIGS:\n",
    "    cfg.DATASET_CONFIGS[\"EXTRACTED_FRAMES\"] = {\n",
    "        \"TYPE\": \"ExtractedFramesTrain\", # We'll use this class\n",
    "        \"CONFIG\": {\n",
    "            \"COMMON\": { # Common params ExtractedFramesTrain might expect via config\n",
    "                \"DATA_IMG_DIR\": EVALUATION_DATA_ROOT,\n",
    "                \"DATA_ANNO_DIR\": EVALUATION_DATA_ROOT, # Not strictly needed if GT loaded by class directly\n",
    "                \"SEQ_LEN\": 5, # This will be used by ExtractedFramesTrain for sequence loading\n",
    "                \"MAX_OBJ_NUM\": cfg.MODEL_MAX_OBJ_NUM,\n",
    "                \"OUTPUT_SIZE\": cfg.DATA_RANDOMCROP, # Example, might need specific eval size\n",
    "            },\n",
    "            \"TRAIN\": { \"RGB\": True } # Example\n",
    "        }\n",
    "    }\n",
    "else: # If it exists, ensure DATA_IMG_DIR is set for our eval data\n",
    "    cfg.DATASET_CONFIGS['EXTRACTED_FRAMES']['CONFIG']['COMMON']['DATA_IMG_DIR'] = EVALUATION_DATA_ROOT\n",
    "    cfg.DATASET_CONFIGS['EXTRACTED_FRAMES']['CONFIG']['COMMON']['DATA_ANNO_DIR'] = EVALUATION_DATA_ROOT\n",
    "    # Important: Ensure SEQ_LEN for ExtractedFramesTrain is what the inference loop expects\n",
    "    # If engine processes frame-by-frame from a sequence, dataloader should provide that sequence.\n",
    "    cfg.DATASET_CONFIGS['EXTRACTED_FRAMES']['CONFIG']['COMMON']['SEQ_LEN'] = cfg.DATA_SEQ_LEN\n",
    "\n",
    "\n",
    "# Display some key config values\n",
    "print(f\"Using checkpoint: {cfg.TEST_CKPT_PATH}\")\n",
    "print(f\"Evaluation on GPU: {cfg.TEST_GPU_ID}\")\n",
    "print(f\"Model: {cfg.MODEL_NAME}\")\n",
    "# print(json.dumps(cfg.__dict__, indent=2, default=str)) # For full config display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Model and Inference Engine Loading\n",
    "from networks.models import build_vos_model\n",
    "from utils.checkpoint import load_network\n",
    "from networks.engines.aot_engine import AOTInferEngine # Ensure this is the correct inference engine\n",
    "\n",
    "# Set device\n",
    "device = torch.device(f\"cuda:{GPU_ID}\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Build model\n",
    "model = build_vos_model(cfg.MODEL_VOS, cfg)\n",
    "\n",
    "# Load fine-tuned weights\n",
    "if cfg.TEST_CKPT_PATH and os.path.exists(cfg.TEST_CKPT_PATH):\n",
    "    model, removed_dict = load_network(model, cfg.TEST_CKPT_PATH, device) # Pass device object\n",
    "    if len(removed_dict) > 0:\n",
    "        print(f\"Removed {len(removed_dict)} keys from checkpoint: {removed_dict}\")\n",
    "    print(f\"Successfully loaded model weights from: {cfg.TEST_CKPT_PATH}\")\n",
    "else:\n",
    "    print(f\"ERROR: Checkpoint not found at {cfg.TEST_CKPT_PATH}. Please set FINETUNED_MODEL_CKPT_PATH correctly.\")\n",
    "    # Potentially raise an error or stop execution\n",
    "    assert False, \"Checkpoint not found.\"\n",
    "\n",
    "model.eval() # Set to evaluation mode\n",
    "model = model.to(device) # Ensure model is on the correct device\n",
    "\n",
    "# Build inference engine\n",
    "# AOTInferEngine(aot_model, gpu_id, long_term_mem_gap, short_term_mem_skip, max_aot_obj_num)\n",
    "# These engine params might need to come from cfg or have sensible defaults.\n",
    "# Based on Evaluator, long_term_mem_gap is cfg.TEST_LONG_TERM_MEM_GAP\n",
    "# short_term_mem_skip is often 1. max_aot_obj_num defaults to model.max_obj_num.\n",
    "engine = AOTInferEngine(\n",
    "    aot_model=model,\n",
    "    gpu_id=GPU_ID, # gpu_id for the engine\n",
    "    long_term_mem_gap=cfg.TEST_LONG_TERM_MEM_GAP if hasattr(cfg, 'TEST_LONG_TERM_MEM_GAP') else 9999,\n",
    "    # short_term_mem_skip=1 # Default in AOTInferEngine if not passed or part of its internal logic\n",
    ")\n",
    "engine.eval() # Engine also has an eval mode\n",
    "# No explicit .cuda() on engine, as AOTInferEngine's constructor takes gpu_id and handles device for its components.\n",
    "# Its internal AOTEngine instances will use the gpu_id.\n",
    "\n",
    "print(\"Model and Inference Engine loaded successfully.\")\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "# For AOTInferEngine, its internal engines are AOTEngine instances which store gpu_id.\n",
    "# We can check one of its AOT model's device if needed after a reference frame is added,\n",
    "# but the main model passed to it is already on the correct device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Dataset and DataLoader for Evaluation\n",
    "from dataloaders.train_datasets import ExtractedFramesTrain # Using the refactored version\n",
    "from dataloaders.video_transforms import ToTensor as VideoToTensor # The custom ToTensor from video_transforms\n",
    "from torchvision import transforms as tv_transforms # Standard torchvision transforms\n",
    "\n",
    "# Define a simple transform for evaluation: just ToTensor and normalization\n",
    "# Note: The custom ToTensor in video_transforms.py also does normalization.\n",
    "# If we need just scaling to [0,1] and then standard Normalize, we might need to adjust.\n",
    "# For now, using the project's custom ToTensor.\n",
    "eval_transforms = tv_transforms.Compose([\n",
    "    VideoToTensor() # This should handle the sample dict and normalize\n",
    "])\n",
    "\n",
    "# Instantiate the dataset\n",
    "# We'll use ExtractedFramesTrain. It will load sequences of cfg.DATA_SEQ_LEN.\n",
    "# The inference engine will then process these frames one by one.\n",
    "eval_dataset = ExtractedFramesTrain(\n",
    "    image_root=EVALUATION_DATA_ROOT, # From Cell 1\n",
    "    transform=eval_transforms,\n",
    "    rgb=cfg.DATASET_CONFIGS['EXTRACTED_FRAMES']['CONFIG']['TRAIN'].get('RGB', True), # from cfg\n",
    "    seq_len=cfg.DATA_SEQ_LEN, # from cfg, e.g., 5\n",
    "    # max_obj_n, repeat_time, ignore_thresh use defaults or values from cfg if needed by __init__\n",
    "    max_obj_n=cfg.MODEL_MAX_OBJ_NUM,\n",
    "    repeat_time=1, # For evaluation, typically process each sequence once\n",
    "    ignore_thresh=cfg.DATASET_CONFIGS['EXTRACTED_FRAMES']['CONFIG']['TRAIN'].get('IGNORE_THRESH', 0.0)\n",
    ")\n",
    "\n",
    "if len(eval_dataset) == 0:\n",
    "    print(f\"WARNING: Evaluation dataset is empty. Check EVALUATION_DATA_ROOT ({EVALUATION_DATA_ROOT}) and seq_len ({cfg.DATA_SEQ_LEN}).\")\n",
    "    # assert False, \"Evaluation dataset is empty.\" # Optionally stop execution\n",
    "\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=1, # Process one sequence at a time\n",
    "    shuffle=False,\n",
    "    num_workers=0, # For simplicity in notebook, can be > 0 if data loading is slow\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Evaluation dataset loaded: {len(eval_dataset)} sequences (or samples).\")\n",
    "# Each sample from this loader will be a sequence of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Inference Loop and Mask Generation\n",
    "# This cell will iterate through sequences, perform inference, and store results.\n",
    "\n",
    "# Lists to store results for later metric calculation and video generation\n",
    "all_gt_masks_for_metrics = [] # List of [ (num_frames-1, H, W), ... ] per sequence for each object\n",
    "all_pred_masks_for_metrics = [] # List of [ (num_frames-1, H, W), ... ] per sequence for each object\n",
    "# For video generation, we might want to store full original images and overlaid masks per sequence\n",
    "# For simplicity now, let's focus on masks for metrics.\n",
    "# We can re-load images for video generation or store paths.\n",
    "\n",
    "# Store paths to original images and predicted masks for video generation for a few examples\n",
    "example_sequence_data = [] # list of dicts: {'seq_name': name, 'frames': [paths], 'gt_masks': [masks], 'pred_masks': [masks]}\n",
    "MAX_EXAMPLE_SEQUENCES_FOR_VIDEO = 3 # Generate video for up to this many sequences\n",
    "\n",
    "print(\"Starting inference...\")\n",
    "for batch_idx, sample_sequence_dict in enumerate(tqdm(eval_dataloader)):\n",
    "    \n",
    "    # Extract data for the current sequence (batch_size is 1)\n",
    "    # Squeeze batch dimension for ref/prev, but keep list for curr\n",
    "    # .to(device) is crucial here if not already done by DataLoader's collate_fn (unlikely for custom dict)\n",
    "    \n",
    "    ref_img_tensor = sample_sequence_dict['ref_img'].to(device)\n",
    "    ref_label_tensor = sample_sequence_dict['ref_label'].to(device).float() # Engine might expect float mask\n",
    "    \n",
    "    prev_img_tensor = sample_sequence_dict['prev_img'].to(device) if 'prev_img' in sample_sequence_dict else None\n",
    "    prev_label_tensor = sample_sequence_dict['prev_label'].to(device).float() if 'prev_label' in sample_sequence_dict else None\n",
    "        \n",
    "    curr_img_tensors = [img.to(device) for img in sample_sequence_dict['curr_img']]\n",
    "    curr_label_tensors = [label.to(device).float() for label in sample_sequence_dict['curr_label']]\n",
    "\n",
    "    seq_meta = sample_sequence_dict['meta']\n",
    "    seq_name = seq_meta['seq_name'][0] if isinstance(seq_meta['seq_name'], list) else seq_meta['seq_name']\n",
    "    \n",
    "    # For AOTInferEngine, obj_nums is a list of integers\n",
    "    obj_nums_list = [seq_meta['obj_num'].item()] if torch.is_tensor(seq_meta['obj_num']) else [seq_meta['obj_num']]\n",
    "\n",
    "\n",
    "    # --- Prepare for AOTInferEngine ---\n",
    "    # The engine processes one frame at a time after an initial reference frame.\n",
    "    # The loaded sample has: ref_img, prev_img, curr_img (list)\n",
    "    # Total frames in this sample = 1 (ref) + 1 (prev) + len(curr_img) = cfg.DATA_SEQ_LEN\n",
    "    \n",
    "    frames_for_engine = [ref_img_tensor, prev_img_tensor] + curr_img_tensors\n",
    "    gt_labels_for_engine = [ref_label_tensor, prev_label_tensor] + curr_label_tensors\n",
    "    \n",
    "    # (Optional) Get original image dimensions if needed for upscaling predictions\n",
    "    # This assumes ToTensor in eval_transforms doesn't alter aspect ratio fundamentally before Resize\n",
    "    # For now, we'll use mask dimensions as they come.\n",
    "    # ori_height = seq_meta.get('original_height', ref_img_tensor.shape[-2]) \n",
    "    # ori_width = seq_meta.get('original_width', ref_img_tensor.shape[-1])\n",
    "\n",
    "    engine.restart_engine()\n",
    "    \n",
    "    # --- Add Reference Frame ---\n",
    "    # Ensure ref_label_tensor is suitable for AOTInferEngine.add_reference_frame\n",
    "    # It expects an integer mask, (B, H, W) or (B, 1, H, W)\n",
    "    # Our labels are (B, 1, H, W), float. Convert to int.\n",
    "    engine.add_reference_frame(\n",
    "        frames_for_engine[0], # ref_img_tensor\n",
    "        gt_labels_for_engine[0].int(),  # ref_label_tensor, converted to int\n",
    "        obj_nums=obj_nums_list \n",
    "    )\n",
    "    \n",
    "    # Store predicted masks for this sequence (excluding reference frame, as per common eval)\n",
    "    current_seq_pred_masks_np = []\n",
    "    current_seq_gt_masks_np = [] # For metrics\n",
    "\n",
    "    # Store data for video if this is an example sequence\n",
    "    save_for_video = (len(example_sequence_data) < MAX_EXAMPLE_SEQUENCES_FOR_VIDEO)\n",
    "    if save_for_video:\n",
    "        video_data_item = {'seq_name': seq_name, 'frames': [], 'pred_masks': [], 'gt_masks': []}\n",
    "        # Store ref frame/mask for video\n",
    "        video_data_item['frames'].append(ref_img_tensor.cpu().squeeze(0).numpy()) # Store as HWC for cv2\n",
    "        video_data_item['pred_masks'].append(ref_label_tensor.cpu().squeeze(0).numpy()) # Use GT as \"pred\" for first frame\n",
    "        video_data_item['gt_masks'].append(ref_label_tensor.cpu().squeeze(0).numpy())\n",
    "\n",
    "\n",
    "    # --- Propagate through subsequent frames ---\n",
    "    # (prev_img is frames_for_engine[1], then curr_imgs start from frames_for_engine[2])\n",
    "    for frame_idx_in_seq in range(1, len(frames_for_engine)):\n",
    "        current_frame_tensor = frames_for_engine[frame_idx_in_seq]\n",
    "        current_gt_label_tensor = gt_labels_for_engine[frame_idx_in_seq] # For metrics\n",
    "\n",
    "        # Predict mask\n",
    "        pred_logits = engine.match_propogate_one_frame(current_frame_tensor) # Output size not specified, uses input size\n",
    "        pred_mask_tensor = engine.predict_current_mask(output_size=current_frame_tensor.shape[-2:]) # Resize to input frame size\n",
    "        \n",
    "        # For multi-object, pred_mask_tensor is (1, H, W). Squeeze batch dim.\n",
    "        pred_mask_np = pred_mask_tensor.squeeze(0).cpu().numpy().astype(np.uint8)\n",
    "        current_seq_pred_masks_np.append(pred_mask_np)\n",
    "        \n",
    "        # Store GT mask for metrics (squeezed, CPU, uint8)\n",
    "        gt_mask_np = current_gt_label_tensor.squeeze(0).cpu().numpy().astype(np.uint8)\n",
    "        current_seq_gt_masks_np.append(gt_mask_np)\n",
    "\n",
    "        # Update engine memory with predicted mask\n",
    "        engine.update_memory(pred_mask_tensor.float()) # Engine might expect float mask\n",
    "\n",
    "        if save_for_video:\n",
    "            video_data_item['frames'].append(current_frame_tensor.cpu().squeeze(0).numpy())\n",
    "            video_data_item['pred_masks'].append(pred_mask_np)\n",
    "            video_data_item['gt_masks'].append(gt_mask_np)\n",
    "            \n",
    "    if current_seq_pred_masks_np: # If there were non-reference frames\n",
    "        # For metrics, typically need (num_objects, num_frames, H, W)\n",
    "        # Assuming single object evaluation for now for simplicity, or main object.\n",
    "        # If multi-object, need to expand/select objects from masks.\n",
    "        # For now, stack them: (num_frames-1, H, W)\n",
    "        # The metric functions (db_eval_iou) will need to handle this.\n",
    "        # Often, evaluation is per object, so we might need to iterate through object IDs.\n",
    "        # Let's assume for now we evaluate the combined mask (object ID 1 if single, or all IDs)\n",
    "        \n",
    "        # Placeholder: for now, we store the raw sequence of masks.\n",
    "        # Metric calculation in next cell will need to process this.\n",
    "        # For simplicity, let's assume we are interested in the primary object or combined mask.\n",
    "        # db_eval_iou expects (num_objects, num_frames, H, W)\n",
    "        # Our masks are (H,W) and we have (num_frames-1) of them.\n",
    "        # We'd need to identify objects and stack them.\n",
    "        # Let's simplify for now: assume metrics will be calculated on combined masks.\n",
    "        all_pred_masks_for_metrics.append(np.stack(current_seq_pred_masks_np, axis=0))\n",
    "        all_gt_masks_for_metrics.append(np.stack(current_seq_gt_masks_np, axis=0))\n",
    "\n",
    "\n",
    "    if save_for_video:\n",
    "        example_sequence_data.append(video_data_item)\n",
    "        \n",
    "    # Optional: Clear CUDA cache if memory is an issue between sequences\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Inference completed.\")\n",
    "if not all_pred_masks_for_metrics:\n",
    "    print(\"WARNING: No predictions were generated. Check dataset and inference loop.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
